{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Skyline EEG data into EEG-BIDS format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. creating folder structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil as sh\n",
    "import csv\n",
    "import pandas as pd\n",
    "from numpy.testing import assert_array_equal\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "from mne_bids import make_bids_folders, make_bids_basename, write_raw_bids\n",
    "from mne_bids.utils import print_dir_tree\n",
    "\n",
    "from mne_bids.copyfiles import copyfile_brainvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = '/home/claire/Documents/STUDY/EEG-Tobacco'\n",
    "orig_data_dir = os.path.join(home, 'DATA')\n",
    "bids_root = os.path.join(home, 'Skyline-EEG-BIDS')\n",
    "if not os.path.exists(bids_root):\n",
    "    os.makedirs(bids_root)\n",
    "        \n",
    "#608, 690,429,558,278,148, 276,647,562,477,703,786,726,743,763,507,388,428,375,747,754,279,572,339,667,594,188,545,681,750,330,753,271,295\n",
    "subject_ids=[]\n",
    "sessions=[1, 2]\n",
    "# event dictionnary\n",
    "trial_type = {'go': 11, 'nogo': 13, 'hw': 21, 'neg': 25, 'neut': 22, 'button_press':8, 'fixation':44}  \n",
    "# will need to add metad data from logfile \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " step1 : rename some data file to match the structure skyline_subject_session :\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed this once, so commenting out now\n",
    "# first let's rename some brainvision datasets that were not correcty labelled\n",
    "\n",
    "#bad_ids = [148, 558, 278, 608]\n",
    "\n",
    "#bad_ids =[690]\n",
    "#for subj in bad_ids:\n",
    "#    for sess in sessions:\n",
    "#        data_path= os.path.join(orig_data_dir, 's%d' %subj,  'session%02d' %sess)\n",
    "#        vhdr_file = os.path.join(data_path, 's%d_%02d.vhdr' %(subj, sess)) \n",
    "        # for s690 use :\n",
    "        #vhdr_file = os.path.join(data_path, 'skyline_%d_%02d.vhdr' %(subj, sess)) \n",
    "#        vhdr_file_renamed = os.path.join(data_path, 'skyline_s%d_%02d.vhdr' %(subj, sess)) \n",
    "#        copyfile_brainvision(vhdr_file, vhdr_file_renamed)\n",
    "#        raw = mne.io.read_raw_brainvision(vhdr_file)\n",
    "#        raw_renamed = mne.io.read_raw_brainvision(vhdr_file_renamed)\n",
    "#        assert_array_equal(raw.get_data(), raw_renamed.get_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b58a84b38dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's%d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'session%02d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# converts data into new BIDS datasets\n",
    "# need to merge info from logfiles with info from events\n",
    "\n",
    "\n",
    "for subj in subject_ids:\n",
    "    for sess in sessions:\n",
    "        data_path= os.path.join(orig_data_dir, 's%d' %subj,  'session%02d' %sess)\n",
    "        fname_in = os.path.join(data_path,'skyline_s%d_%02d.vhdr' %(subj, sess))  \n",
    "        raw = mne.io.read_raw_brainvision(fname_in, preload=False)\n",
    "        \n",
    "       \n",
    "        events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "        bids_basename = make_bids_basename(subject=str(subj), session = str(sess))\n",
    "\n",
    "        write_raw_bids(raw, bids_basename, bids_root, event_id=trial_type,\n",
    "               events_data=events, overwrite=True)\n",
    "        \n",
    "        # read events and logfile files\n",
    "        events_file = os.path.join(bids_root,'sub-%d' %subj, 'ses-%d' %sess, 'eeg','sub-%d_' %subj + 'ses-%d_events.tsv' %sess) \n",
    "\n",
    "        #read logfile\n",
    "        logfile_file = os.path.join(data_path, 's%d_%02d_logfile.txt' %(subj, sess) )\n",
    "\n",
    "        #output_file= os.path.join((bids_root,'sub-%d' %subj, 'ses-%d' %sess, 'eeg','sub-%d_' %subj + 'ses-%d_events.tsv' %sess) \n",
    "\n",
    "\n",
    "        MAPPINGS = {\"n/a\": None,\n",
    "                    \"fixation\": None,\n",
    "                    \"neg\": [\"Neg\"],\n",
    "                    \"neut\": [\"Neut\"],\n",
    "                    \"hw\": [\"HW\"],\n",
    "                    \"go\": [\"HW\", \"SmoCuDa\"],\n",
    "                    \"nogo\": [\"HW\", \"SmoCuDa\"],\n",
    "                    \"button_press\": None\n",
    "                    }\n",
    "\n",
    "        csv_details = None\n",
    "        last_idx = 1\n",
    "        new_row_list =[]\n",
    "        \n",
    "        with open(logfile_file,'r') as csvfile:\n",
    "            csv_details = list(csv.reader(csvfile, delimiter='\\t'))\n",
    "\n",
    "        def get_file_details(trial_type):\n",
    "            global last_idx\n",
    "            if MAPPINGS[trial_type] is None:\n",
    "                return None\n",
    "\n",
    "            for idx, row in enumerate(list(csv_details)[last_idx:]):\n",
    "                for ttype in MAPPINGS[trial_type]:\n",
    "                    if ttype in row[1]:\n",
    "                        last_idx += idx + 1\n",
    "                        return \"_\".join(row[1].split(\"_\")[-2:])\n",
    "\n",
    "        with open(events_file,'r') as csvfile:\n",
    "                events_reader = csv.reader(csvfile, delimiter='\\t')\n",
    "                for row in list(events_reader)[1:]:\n",
    "                    csvline = ','.join(row)\n",
    "                    csvline += ','\n",
    "\n",
    "                    filename = get_file_details(row[2])\n",
    "                    if filename:\n",
    "                        csvline += filename\n",
    "\n",
    "                    new_row_list.append(csvline)\n",
    "                    print(csvline)\n",
    "\n",
    "\n",
    "        # create and save dataframe (replaces previous events.tsv file with added column for filename)\n",
    "        df_full = pd.DataFrame([sub.split(\",\") for sub in new_row_list], columns =['onset', 'duration', 'trial_type', 'value', 'sample', 'filename' ] )\n",
    "        df_full.to_csv(events_file, sep = '\\t')\n",
    "        \n",
    "        del df_full\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge info from events.tsv and logfile.text to get full events meta-data in one single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
